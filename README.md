# Facebook Marketplace ETL Pipeline - Data Engineering Project

Complete end-to-end data pipeline for scraping, processing, and analyzing Facebook Marketplace data using Apache Airflow, MinIO, and PostgreSQL.

## ğŸ“ Course Requirements Coverage

This project fulfills all Data Engineering course requirements:

### âœ… 1. Data Acquisition
- **Method:** Web Scraping using Selenium + BeautifulSoup
- **Source:** Facebook Marketplace Thailand (https://facebook.com/marketplace)
- **Ethical Considerations:**
  - Respects rate limiting (2-second delays between requests)
  - Only scrapes publicly available data
  - No personal information collected
  - Follows robots.txt guidelines

### âœ… 2. Data Storage (Multi-tier Architecture)
- **Raw Data:** MinIO/S3 (CSV format)
  - Human-readable for debugging
  - Easy to inspect and validate
- **Processed Data:** Parquet format (MinIO/S3)
  - Column-oriented storage
  - Snappy compression
  - Optimized for analytics queries
- **Structured Data:** PostgreSQL database
  - ACID compliance
  - Complex querying support
  - Data integrity enforcement

### âœ… 3. Data Cleaning & Transformation (Pandas)
Comprehensive 8-step data quality pipeline:
1. **Missing Value Handling:** Drop critical nulls, fill non-critical fields
2. **Data Type Correction:** Parse dates, convert numeric fields
3. **Price Data Cleaning:** Extract numeric values from Thai Baht strings using regex
4. **Text Standardization:** Clean titles, standardize locations, lowercase normalization
5. **Feature Engineering:** Extract phone models, create price categories
6. **Deduplication:** Remove duplicate URLs while preserving first occurrence
7. **Outlier Detection:** Flag price outliers using quantile-based method
8. **Data Validation:** URL validation, length checks, schema enforcement

### âœ… 4. Data Loading & Verification
- **Multi-format Storage:** Parquet, CSV, PostgreSQL
- **Quality Checks:**
  - Row count validation
  - Schema verification
  - Data type consistency
  - Summary statistics generation
  - Completeness metrics
  - Outlier reporting

### âœ… 5. Reproducibility & Containerization
- **Docker Compose:** Multi-service orchestration
- **Custom Dockerfile:** Airflow with Chrome/Selenium
- **Environment Variables:** `.env` configuration
- **requirements.txt:** All Python dependencies
- **Documentation:** Comprehensive setup instructions

### âœ… 6. Advanced Components
- **Apache Airflow:** Complete DAG orchestration with error handling
- **Workflow Management:** Branching, retry logic, trigger rules
- **Production Ready:** Logging, monitoring, cleanup tasks

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Apache Airflow                           â”‚
â”‚  (Workflow Orchestration & Scheduling)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Scrape â”‚   â”‚  Transform  â”‚   â”‚ Verify â”‚
â”‚  Data  â”‚   â”‚  & Clean    â”‚   â”‚ Qualityâ”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚               â”‚
    â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚             â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MinIO  â”‚ â”‚ MinIO  â”‚ â”‚ PostgreSQL â”‚
â”‚  (Raw) â”‚ â”‚(Parquetâ”‚ â”‚  (Table)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—‚ï¸ Project Structure

```
airflow-minio-project/
â”œâ”€â”€ dags/                                    # Airflow DAG files
â”‚   â”œâ”€â”€ complete_etl_pipeline_dag.py        # â­ Main project (all requirements)
â”‚   â”œâ”€â”€ marketplace_advanced_flow_dag.py    # Advanced parallel workflow
â”‚   â”œâ”€â”€ marketplace_scraper_detailed_dag.py # Detailed scraper with dedup
â”‚   â””â”€â”€ etl_pipeline_dag.py                 # Legacy ETL example
â”œâ”€â”€ logs/                                    # Airflow execution logs
â”œâ”€â”€ plugins/                                 # Custom Airflow plugins
â”œâ”€â”€ config/                                  # Configuration files
â”œâ”€â”€ scripts/                                 # Utility scripts
â”‚   â”œâ”€â”€ init-airflow.sh                     # Airflow initialization
â”‚   â””â”€â”€ test-minio.py                       # MinIO connection test
â”œâ”€â”€ docker-compose.yml                       # Multi-service orchestration
â”œâ”€â”€ Dockerfile                               # Custom Airflow image with Chrome
â”œâ”€â”€ requirements.txt                         # Python dependencies
â”œâ”€â”€ .env                                     # Environment variables
â”œâ”€â”€ README.md                                # This file
â”œâ”€â”€ QUICKSTART.md                            # Quick start guide
â”œâ”€â”€ FILE_STRUCTURE.md                        # Detailed structure docs
â””â”€â”€ AIRFLOW_VARIABLES_SETUP.md              # Credentials setup guide
```

## ğŸš€ à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™

### 1. à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™ Airflow + MinIO

```bash
# à¹€à¸‚à¹‰à¸²à¹„à¸›à¸—à¸µà¹ˆà¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹‚à¸›à¸£à¹€à¸ˆà¸„
cd airflow-minio-project

# Start services
docker-compose up -d

# à¸”à¸¹ logs
docker-compose logs -f
```

### 2. à¹€à¸‚à¹‰à¸²à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ Web UI

**Airflow Web UI:**
- URL: http://localhost:8080
- Username: `airflow`
- Password: `airflow`

**MinIO Console:**
- URL: http://localhost:9001
- Username: `minioadmin`
- Password: `minioadmin`

### 3. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ªà¸–à¸²à¸™à¸°

```bash
# à¸”à¸¹à¸ªà¸–à¸²à¸™à¸° containers
docker-compose ps

# à¸”à¸¹ logs à¸‚à¸­à¸‡ Airflow
docker-compose logs airflow-webserver
docker-compose logs airflow-scheduler

# à¸”à¸¹ logs à¸‚à¸­à¸‡ MinIO
docker-compose logs minio
```

### 4. à¸«à¸¢à¸¸à¸”à¹à¸¥à¸°à¸¥à¸š services

```bash
# à¸«à¸¢à¸¸à¸” services
docker-compose down

# à¸«à¸¢à¸¸à¸”à¹à¸¥à¸°à¸¥à¸š volumes (à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸°à¸«à¸²à¸¢à¸«à¸¡à¸”!)
docker-compose down -v
```

## ğŸ“Š Available DAGs

### 1. `complete_etl_pipeline` â­ **Main Project DAG**
- **Schedule:** Every 6 hours
- **Description:** Complete ETL pipeline meeting all course requirements
- **Pipeline Stages:**
  1. **Data Acquisition:** Scrape Facebook Marketplace
  2. **Store Raw Data:** Save to MinIO (CSV)
  3. **Clean & Transform:** 8-step Pandas pipeline
  4. **Store Processed:** Parquet + PostgreSQL
  5. **Verify Quality:** Statistics & validation
- **Features:**
  - Comprehensive data cleaning
  - Multi-format storage
  - Quality verification
  - Error handling & retries
  - Detailed logging

### 2. `marketplace_advanced_flow`
- **Schedule:** Every 6 hours
- **Description:** Advanced workflow with parallel scraping
- **Features:**
  - Parallel keyword scraping
  - Data validation branching
  - Smart storage (by keyword)
  - Auto cleanup (30-day retention)
  - Failure handling

### 3. `marketplace_scraper_with_details`
- **Schedule:** Hourly
- **Description:** Detailed scraper with deduplication
- **Features:**
  - Login support (Airflow Variables)
  - Detail fetching (condition, description)
  - Smart deduplication (insert/update/skip)
  - Hourly files (replace mode)
  - Daily files (append mode)

### 4. `etl_pipeline_minio` (Legacy)
- **Schedule:** Hourly
- **Description:** Basic ETL example
- **Tasks:** Extract â†’ Transform â†’ Load â†’ Report

## ğŸ—‚ï¸ MinIO Storage Structure

```
marketplace-data/
â”œâ”€â”€ raw/                           # Raw scraped data (CSV)
â”‚   â””â”€â”€ marketplace_raw_YYYYMMDD_HHMMSS.csv
â”œâ”€â”€ processed/                     # Cleaned & transformed data
â”‚   â”œâ”€â”€ marketplace_clean_YYYYMMDD_HHMMSS.parquet  # Analytics format
â”‚   â””â”€â”€ marketplace_clean_YYYYMMDD_HHMMSS.csv      # Compatibility
â”œâ”€â”€ hourly/                        # Hourly snapshots (replace mode)
â”‚   â””â”€â”€ marketplace_YYYYMMDD_HH.csv
â”œâ”€â”€ daily/                         # Daily cumulative (append mode)
â”‚   â””â”€â”€ marketplace_YYYYMMDD.csv
â””â”€â”€ by_keyword/                    # Organized by search keyword
    â”œâ”€â”€ iphone_13_YYYYMMDD_HHMMSS.csv
    â””â”€â”€ samsung_galaxy_YYYYMMDD_HHMMSS.csv
```

## ğŸ“Š PostgreSQL Schema

```sql
CREATE TABLE marketplace_listings (
    id SERIAL PRIMARY KEY,
    url TEXT UNIQUE NOT NULL,
    title TEXT,
    title_clean TEXT,
    phone_model VARCHAR(50),
    price VARCHAR(50),
    price_numeric DECIMAL(10,2),
    price_category VARCHAR(50),
    is_price_outlier BOOLEAN,
    location TEXT,
    location_clean VARCHAR(100),
    keyword VARCHAR(100),
    search_location VARCHAR(100),
    radius_km INTEGER,
    image_url TEXT,
    scraped_at TIMESTAMP,
    days_since_scraped INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## ğŸ”§ Configuration

### Airflow Variables (Required for Scraping with Login)

Set these in Airflow UI: Admin â†’ Variables

```
Key: fb_marketplace_email
Value: your.email@example.com

Key: fb_marketplace_password
Value: your_password
```

See [AIRFLOW_VARIABLES_SETUP.md](AIRFLOW_VARIABLES_SETUP.md) for detailed instructions.

### MinIO Connection (Auto-configured)

Already configured in `docker-compose.yml`:
- **Conn Id:** `minio_default`
- **Conn Type:** `AWS`
- **Access Key:** `minioadmin`
- **Secret Key:** `minioadmin`
- **Endpoint:** `http://minio:9000`

### PostgreSQL Connection (Auto-configured)

Already configured in `docker-compose.yml`:
- **Conn Id:** `postgres_default`
- **Host:** `postgres`
- **Database:** `airflow`
- **User:** `airflow`
- **Password:** `airflow`
- **Port:** `5432`

## ğŸ¯ Running the Main ETL Pipeline

### 1. Start Services

```bash
cd airflow-minio-project
docker-compose up -d
```

### 2. Access Airflow UI

Open http://localhost:8080
- Username: `airflow`
- Password: `airflow`

### 3. Configure DAG Parameters (Optional)

Click on `complete_etl_pipeline` DAG â†’ Trigger DAG with config:

```json
{
  "keyword": "iphone 13",
  "location": "bangkok",
  "radius": 20,
  "max_items": 50
}
```

### 4. Monitor Execution

- View logs for each task
- Check data in MinIO Console (http://localhost:9001)
- Query PostgreSQL data

### 5. View Results

**MinIO Console:** http://localhost:9001
- Browse `marketplace-data` bucket
- Download Parquet/CSV files

**PostgreSQL:**
```bash
docker exec -it airflow-minio-project-postgres-1 psql -U airflow -d airflow
```

```sql
-- View scraped data
SELECT * FROM marketplace_listings ORDER BY scraped_at DESC LIMIT 10;

-- Price statistics
SELECT 
    phone_model,
    COUNT(*) as count,
    AVG(price_numeric) as avg_price,
    MIN(price_numeric) as min_price,
    MAX(price_numeric) as max_price
FROM marketplace_listings
WHERE price_numeric IS NOT NULL
GROUP BY phone_model
ORDER BY count DESC;
```
    
    # Upload file
    s3_hook.load_string(
        string_data="Hello MinIO!",
        key="my_file.txt",
        bucket_name="raw-data"
    )
    
    print("âœ… Uploaded successfully!")

with DAG(
    'my_new_dag',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False,
) as dag:
    
    task = PythonOperator(
        task_id='upload_task',
        python_callable=my_function,
    )
```

## ğŸ› Troubleshooting

### à¸›à¸±à¸à¸«à¸²: Airflow à¹„à¸¡à¹ˆà¹€à¸«à¹‡à¸™ DAGs

```bash
# à¹€à¸Šà¹‡à¸„à¸§à¹ˆà¸² DAG files à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¸—à¸µà¹ˆà¸–à¸¹à¸à¸•à¹‰à¸­à¸‡
ls -la dags/

# à¸”à¸¹ logs à¸‚à¸­à¸‡ scheduler
docker-compose logs airflow-scheduler
```

### à¸›à¸±à¸à¸«à¸²: à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­ MinIO

```bash
# à¹€à¸Šà¹‡à¸„ MinIO service
docker-compose ps minio

# à¸—à¸”à¸ªà¸­à¸šà¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­
curl http://localhost:9000/minio/health/live
```

### à¸›à¸±à¸à¸«à¸²: Permission denied

```bash
# à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² permissions
chmod -R 777 logs/
chmod -R 777 dags/
chmod -R 777 plugins/
```

## ğŸ“š Resources

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)
- [Airflow S3 Provider](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/index.html)

## ğŸ” Security Notes

**âš ï¸ à¸ªà¸³à¸„à¸±à¸:** à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸™à¸µà¹‰à¹ƒà¸Šà¹‰ credentials à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸à¸±à¸’à¸™à¸²à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™

à¸ªà¸³à¸«à¸£à¸±à¸š Production:
- à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ passwords à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
- à¹ƒà¸Šà¹‰ secret management (Vault, AWS Secrets Manager)
- à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² SSL/TLS
- à¸à¸³à¸«à¸™à¸” network policies
- à¹€à¸›à¸´à¸” authentication à¹à¸¥à¸° authorization

## ğŸ“„ License

This project is for educational purposes.

---

**Created:** November 2025
**Author:** team Cheesedip
